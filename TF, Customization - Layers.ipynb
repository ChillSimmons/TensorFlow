{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 2.0 alpha - Custom Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "#### For many Machine Learning operations higher level abstraction is needed - Not specification of individual variables/operations\n",
    "#### TF provides many simple, common layers capable of ML operations - As well as the ability to write new, or mixed-existing layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers are Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First argument for most, is # of output dimensions\n",
    "\n",
    "layer = tf.keras.layers.Dense(100)\n",
    "\n",
    "# Input argument, often implied, can be used as well\n",
    "\n",
    "layer = tf.keras.layers.Dense(10, input_shape=(None, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are many pre-existing Layers - including Dense (fully-connected layer), Conv2D, LSTM, BatchNormalization, Dropout, etc.\n",
    "### To use Layer, Call it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=59, shape=(10, 10), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(tf.zeros([10, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect layer Variables with layer.variable - and trainable variables, with layer.trainable_variables\n",
    "#### Fully-connected layers - variables for weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_3/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
       " array([[ 0.25257814,  0.01163477, -0.24498141,  0.09267968,  0.41632444,\n",
       "          0.32417148, -0.21949351,  0.14460891, -0.2771877 ,  0.11145324],\n",
       "        [ 0.5078184 ,  0.52482957, -0.11163193, -0.12754756,  0.02461725,\n",
       "          0.09317684,  0.1122179 , -0.02014148, -0.27136603,  0.08979642],\n",
       "        [-0.19607595,  0.06553143, -0.25996608, -0.40616053,  0.12482899,\n",
       "          0.00111449,  0.27387106,  0.23684663, -0.46238   , -0.20166436],\n",
       "        [-0.02108878,  0.4665267 , -0.615757  ,  0.48804277, -0.18181401,\n",
       "          0.22222596, -0.5824282 ,  0.60214454,  0.2684703 ,  0.603854  ],\n",
       "        [-0.59859943,  0.23128837, -0.52116936,  0.46380764,  0.15718937,\n",
       "         -0.28825155, -0.16088012, -0.12785834, -0.40382934, -0.39580446]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_3/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables are also accessible through accessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'dense_3/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
       " array([[ 0.25257814,  0.01163477, -0.24498141,  0.09267968,  0.41632444,\n",
       "          0.32417148, -0.21949351,  0.14460891, -0.2771877 ,  0.11145324],\n",
       "        [ 0.5078184 ,  0.52482957, -0.11163193, -0.12754756,  0.02461725,\n",
       "          0.09317684,  0.1122179 , -0.02014148, -0.27136603,  0.08979642],\n",
       "        [-0.19607595,  0.06553143, -0.25996608, -0.40616053,  0.12482899,\n",
       "          0.00111449,  0.27387106,  0.23684663, -0.46238   , -0.20166436],\n",
       "        [-0.02108878,  0.4665267 , -0.615757  ,  0.48804277, -0.18181401,\n",
       "          0.22222596, -0.5824282 ,  0.60214454,  0.2684703 ,  0.603854  ],\n",
       "        [-0.59859943,  0.23128837, -0.52116936,  0.46380764,  0.15718937,\n",
       "         -0.28825155, -0.16088012, -0.12785834, -0.40382934, -0.39580446]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_3/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.kernel, layer.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Custom Layers\n",
    "### Extend tf.keras.layers class, implementing:\n",
    "#### __init__ - can do all input-independent initialization - required Shapes for variables must be specified\n",
    "#### __build__ - when creating variables after learning the input Shapes - *Late* variable creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n",
      "[<tf.Variable 'my_dense_layer/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
      "array([[-0.31676382, -0.35606778, -0.55156136,  0.48472935, -0.26616472,\n",
      "         0.56535727,  0.34881133, -0.03091329,  0.5743274 , -0.3905552 ],\n",
      "       [-0.32543886, -0.58241904,  0.25177127,  0.16202146,  0.518887  ,\n",
      "        -0.25680554, -0.17120555, -0.5065768 , -0.12480593, -0.2116181 ],\n",
      "       [-0.4962682 ,  0.5740709 , -0.41251507,  0.1451804 ,  0.5029914 ,\n",
      "         0.09180254, -0.3872588 , -0.23413709, -0.6006301 , -0.36873376],\n",
      "       [-0.02486002,  0.34510267, -0.4848554 , -0.03226358,  0.18390155,\n",
      "         0.62642556, -0.17898566, -0.44808623,  0.52434057,  0.0689016 ],\n",
      "       [ 0.49699193,  0.37744623, -0.14436734, -0.24741817,  0.51523465,\n",
      "        -0.3198666 , -0.38199866,  0.35677677, -0.14090943, -0.10732716]],\n",
      "      dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_outputs):\n",
    "        super(MyDenseLayer, self).__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_variable('kernel', shape=[int(input_shape[-1]),\n",
    "                                                        self.num_outputs])\n",
    "    \n",
    "    def call(self, input):\n",
    "        return tf.matmul(input, self.kernel)\n",
    "\n",
    "layer = MyDenseLayer(10)\n",
    "\n",
    "print(layer(tf.zeros([10,5])))\n",
    "print(layer.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### call - does the forward computation\n",
    "# Models - Composing Layers\n",
    "#### Use tf.keras.Model - to create a layer-like container of other layers\n",
    "#### example - resnet, with each residual block containing convolutions, batch normalizations, and a shortcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]]], shape=(1, 2, 3, 3), dtype=float32)\n",
      "['resnet_identity_block/conv2d/kernel:0', 'resnet_identity_block/conv2d/bias:0', 'resnet_identity_block/batch_normalization_v2/gamma:0', 'resnet_identity_block/batch_normalization_v2/beta:0', 'resnet_identity_block/conv2d_1/kernel:0', 'resnet_identity_block/conv2d_1/bias:0', 'resnet_identity_block/batch_normalization_v2_1/gamma:0', 'resnet_identity_block/batch_normalization_v2_1/beta:0', 'resnet_identity_block/conv2d_2/kernel:0', 'resnet_identity_block/conv2d_2/bias:0', 'resnet_identity_block/batch_normalization_v2_2/gamma:0', 'resnet_identity_block/batch_normalization_v2_2/beta:0']\n"
     ]
    }
   ],
   "source": [
    "class ResnetIdentityBlock(tf.keras.Model):\n",
    "    def __init__(self, kernel_size, filters):\n",
    "        super(ResnetIdentityBlock, self).__init__(name='')\n",
    "        filters1, filters2, filters3 = filters\n",
    "        \n",
    "        self.conv2a = tf.keras.layers.Conv2D(filters1, (1, 1))\n",
    "        self.bn2a = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv2b = tf.keras.layers.Conv2D(filters2, kernel_size, padding='same')\n",
    "        self.bn2b = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv2c = tf.keras.layers.Conv2D(filters3, (1, 1))\n",
    "        self.bn2c = tf.keras.layers.BatchNormalization()\n",
    "    \n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.conv2a(input_tensor)\n",
    "        x = self.bn2a(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.conv2b(x)\n",
    "        x = self.bn2b(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.conv2c(x)\n",
    "        x = self.bn2c(x, training=training)\n",
    "        \n",
    "        x += input_tensor\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "block = ResnetIdentityBlock(1, [1, 2, 3])\n",
    "\n",
    "print(block(tf.zeros([1, 2, 3, 3])))\n",
    "print([x.name for x in block.trainable_variables])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use tf.keras.Sequential - for other models with layers, where simply one is called after another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=756, shape=(1, 2, 3, 3), dtype=float32, numpy=\n",
       "array([[[[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_seq = tf.keras.Sequential([tf.keras.layers.Conv2D(1, (1,1), input_shape=(None,None,3)),\n",
    "                             tf.keras.layers.BatchNormalization(),\n",
    "                             tf.keras.layers.Conv2D(2, 1, padding='same'),\n",
    "                             tf.keras.layers.BatchNormalization(),\n",
    "                             tf.keras.layers.Conv2D(3, (1, 1)),\n",
    "                             tf.keras.layers.BatchNormalization()])\n",
    "my_seq(tf.zeros([1,2,3,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "### These methods will allow for adapting models, using custom layers, or models, for better structure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
