{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 2.0 alpha - Saving & Restoring Models\n",
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model progress can be saved during Training, as well as after - the model can pause and resume to avoid long training times - This also allows for the model to be shared with others\n",
    "#### Different TF model APIs require different saving methods - This exercise uses tf.keras API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q h5py pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting msgpack\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/9c/0036c66234482044070836cc622266839e2412f8108849ab0bfdeaab8578/msgpack-0.6.1.tar.gz (118kB)\n",
      "\u001b[K    100% |████████████████████████████████| 122kB 1.7MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: msgpack\n",
      "  Running setup.py bdist_wheel for msgpack ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/MPHA/Library/Caches/pip/wheels/e0/eb/73/79c4057260fcb51c5f12cee027dda5cf79b92b618a82529c74\n",
      "Successfully built msgpack\n",
      "Installing collected packages: msgpack\n",
      "Successfully installed msgpack-0.6.1\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install msgpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Dataset - Demonstrate Saving Weights\n",
    "#### Use only the first 1000 examples from the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 11s 1us/step\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data() \n",
    "\n",
    "train_labels = train_labels[:1000]\n",
    "test_labels = test_labels[:1000]\n",
    "\n",
    "train_images = train_images[:1000].reshape(-1, 28 * 28)/255.0\n",
    "test_images = test_images[:1000].reshape(-1, 28 * 28)/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Return a short Sequential model\n",
    "\n",
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        keras.layers.Dense(512, activation='relu', input_shape=(784,)),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create a basic model Instance\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Checkpoints during Training\n",
    "#### Typical usage involves automatically saving checkpoints During training, and at the End - the model can be used at a later time without retraining it, and pick up where it left off if training was interrupted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      " 832/1000 [=======================>......] - ETA: 0s - loss: 1.2674 - accuracy: 0.6274\n",
      "Epoch 00001: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 1s 694us/sample - loss: 1.1700 - accuracy: 0.6540 - val_loss: 0.7452 - val_accuracy: 0.7750\n",
      "Epoch 2/10\n",
      " 768/1000 [======================>.......] - ETA: 0s - loss: 0.4607 - accuracy: 0.8646\n",
      "Epoch 00002: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 462us/sample - loss: 0.4423 - accuracy: 0.8690 - val_loss: 0.5537 - val_accuracy: 0.8370\n",
      "Epoch 3/10\n",
      " 768/1000 [======================>.......] - ETA: 0s - loss: 0.3035 - accuracy: 0.9219\n",
      "Epoch 00003: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 443us/sample - loss: 0.2952 - accuracy: 0.9230 - val_loss: 0.4688 - val_accuracy: 0.8510\n",
      "Epoch 4/10\n",
      " 960/1000 [===========================>..] - ETA: 0s - loss: 0.2202 - accuracy: 0.9469\n",
      "Epoch 00004: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 393us/sample - loss: 0.2168 - accuracy: 0.9470 - val_loss: 0.4380 - val_accuracy: 0.8590\n",
      "Epoch 5/10\n",
      " 992/1000 [============================>.] - ETA: 0s - loss: 0.1644 - accuracy: 0.9627\n",
      "Epoch 00005: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 368us/sample - loss: 0.1639 - accuracy: 0.9630 - val_loss: 0.4417 - val_accuracy: 0.8640\n",
      "Epoch 6/10\n",
      " 992/1000 [============================>.] - ETA: 0s - loss: 0.1150 - accuracy: 0.9768\n",
      "Epoch 00006: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 354us/sample - loss: 0.1146 - accuracy: 0.9770 - val_loss: 0.4324 - val_accuracy: 0.8600\n",
      "Epoch 7/10\n",
      " 928/1000 [==========================>...] - ETA: 0s - loss: 0.0920 - accuracy: 0.9838\n",
      "Epoch 00007: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 312us/sample - loss: 0.0903 - accuracy: 0.9840 - val_loss: 0.4170 - val_accuracy: 0.8640\n",
      "Epoch 8/10\n",
      " 768/1000 [======================>.......] - ETA: 0s - loss: 0.0684 - accuracy: 0.9922\n",
      "Epoch 00008: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 321us/sample - loss: 0.0667 - accuracy: 0.9930 - val_loss: 0.4153 - val_accuracy: 0.8620\n",
      "Epoch 9/10\n",
      " 992/1000 [============================>.] - ETA: 0s - loss: 0.0498 - accuracy: 0.9960\n",
      "Epoch 00009: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 339us/sample - loss: 0.0496 - accuracy: 0.9960 - val_loss: 0.4236 - val_accuracy: 0.8680\n",
      "Epoch 10/10\n",
      " 768/1000 [======================>.......] - ETA: 0s - loss: 0.0416 - accuracy: 0.9948\n",
      "Epoch 00010: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 332us/sample - loss: 0.0405 - accuracy: 0.9960 - val_loss: 0.4204 - val_accuracy: 0.8680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb27e979e8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model and pass the ModelCheckpoint Callback\n",
    "\n",
    "checkpoint_path = 'training_1/cp.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create callback\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                               save_weights_only=True,\n",
    "                                               verbose=1)\n",
    "model = create_model()\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=10,\n",
    "         validation_data = (test_images, test_labels),\n",
    "         callbacks = [cp_callback])         # pass callback to training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This creates a single collection of TF Checkpoint files - updated after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint                  cp.ckpt.index\r\n",
      "cp.ckpt.data-00000-of-00001\r\n"
     ]
    }
   ],
   "source": [
    "!ls {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an untrained model - Restore from the Weights\n",
    "#### If restoring a model from Weights alone, must use a model of the same architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 141us/sample - loss: 2.3714 - accuracy: 0.1120\n",
      "Untrained model, accuracy: 11.20%\n"
     ]
    }
   ],
   "source": [
    "# Create Untrained model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "loss, acc = model.evaluate(test_images, test_labels)\n",
    "print('Untrained model, accuracy: {:5.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 109us/sample - loss: 0.4204 - accuracy: 0.8680\n",
      "Restored model, accuracy: 86.80%\n"
     ]
    }
   ],
   "source": [
    "# Load the Weights\n",
    "\n",
    "model.load_weights(checkpoint_path)\n",
    "loss, acc = model.evaluate(test_images, test_labels)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once again - 86.80 % accuracy achieved\n",
    "## Checkpoint Callback Options - Names & Frequency\n",
    "#### Train a New Model using these Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00005: saving model to training_2/cp-0005.ckpt\n",
      "\n",
      "Epoch 00010: saving model to training_2/cp-0010.ckpt\n",
      "\n",
      "Epoch 00015: saving model to training_2/cp-0015.ckpt\n",
      "\n",
      "Epoch 00020: saving model to training_2/cp-0020.ckpt\n",
      "\n",
      "Epoch 00025: saving model to training_2/cp-0025.ckpt\n",
      "\n",
      "Epoch 00030: saving model to training_2/cp-0030.ckpt\n",
      "\n",
      "Epoch 00035: saving model to training_2/cp-0035.ckpt\n",
      "\n",
      "Epoch 00040: saving model to training_2/cp-0040.ckpt\n",
      "\n",
      "Epoch 00045: saving model to training_2/cp-0045.ckpt\n",
      "\n",
      "Epoch 00050: saving model to training_2/cp-0050.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb2a388160>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a new model - saving uniquely Named Checkpoints, every 5 epochs\n",
    "\n",
    "checkpoint_path = 'training_2/cp-{epoch:04d}.ckpt'    # include epoch in file name\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                verbose=1,\n",
    "                                                save_weights_only=True,   # save weights\n",
    "                                                period=5)                 # every 5 epochs\n",
    "model = create_model()\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "model.fit(train_images, train_labels,\n",
    "         epochs = 50, callbacks = [cp_callback],\n",
    "         validation_data = (test_images, test_labels),\n",
    "         verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Of the Resulting Checkpoints above - Choose the Latest one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint                       cp-0025.ckpt.index\r\n",
      "cp-0000.ckpt.data-00000-of-00001 cp-0030.ckpt.data-00000-of-00001\r\n",
      "cp-0000.ckpt.index               cp-0030.ckpt.index\r\n",
      "cp-0005.ckpt.data-00000-of-00001 cp-0035.ckpt.data-00000-of-00001\r\n",
      "cp-0005.ckpt.index               cp-0035.ckpt.index\r\n",
      "cp-0010.ckpt.data-00000-of-00001 cp-0040.ckpt.data-00000-of-00001\r\n",
      "cp-0010.ckpt.index               cp-0040.ckpt.index\r\n",
      "cp-0015.ckpt.data-00000-of-00001 cp-0045.ckpt.data-00000-of-00001\r\n",
      "cp-0015.ckpt.index               cp-0045.ckpt.index\r\n",
      "cp-0020.ckpt.data-00000-of-00001 cp-0050.ckpt.data-00000-of-00001\r\n",
      "cp-0020.ckpt.index               cp-0050.ckpt.index\r\n",
      "cp-0025.ckpt.data-00000-of-00001\r\n"
     ]
    }
   ],
   "source": [
    "!ls {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training_2/cp-0050.ckpt'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test - reset the Model, Loading the Latest Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 156us/sample - loss: 0.4834 - accuracy: 0.8770\n",
      "Restored model, accuracy: 87.70%\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.load_weights(latest)\n",
    "loss, acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 159us/sample - loss: 0.4834 - accuracy: 0.8770\n",
      "Restored model, accuracy: 87.70%\n"
     ]
    }
   ],
   "source": [
    "# save the weights\n",
    "\n",
    "model.save_weights('./checkpoints/my_checkpoint')\n",
    "\n",
    "# restore the weights\n",
    "\n",
    "model = create_model()\n",
    "model.load_weights('./checkpoints/my_checkpoint')\n",
    "\n",
    "loss, acc = model.evaluate(test_images, test_labels)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Entire Model\n",
    "#### The model configuration, weights, and variables can be saved, allowing access without having the original python code - Models can be loaded in Tensorflow.js (HDF5, saved model), then train them to be run in Web Browsers or convert them to run on Mobile devices using Tensorflow LIte (HDF5, saved model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras provides basic save format using the HDF5 standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 363us/sample - loss: 1.1440 - accuracy: 0.6700\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 278us/sample - loss: 0.4149 - accuracy: 0.8860\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 258us/sample - loss: 0.2846 - accuracy: 0.9200\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 238us/sample - loss: 0.2042 - accuracy: 0.9520\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 234us/sample - loss: 0.1525 - accuracy: 0.9700\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.fit(train_images, train_labels, epochs=5)\n",
    "\n",
    "# save entire model to a HDF5 file\n",
    "\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreate the Model from that File - including weights & optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = keras.models.load_model('my_model.h5')\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 172us/sample - loss: 0.4438 - accuracy: 0.8500\n",
      "Restored model, accuracy: 85.00%\n"
     ]
    }
   ],
   "source": [
    "# Check the Accuracy\n",
    "\n",
    "loss, acc = new_model.evaluate(test_images, test_labels)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras saves the Weight values, Model configuration, and Optimizer configuration by inspecting model architecture - it is NOT currently able to save TF optimizers (from tf.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save_model - an additional (experimental) method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 432us/sample - loss: 1.1478 - accuracy: 0.6810\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 277us/sample - loss: 0.4260 - accuracy: 0.8840\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 305us/sample - loss: 0.2807 - accuracy: 0.9250\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 297us/sample - loss: 0.1986 - accuracy: 0.9560\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 224us/sample - loss: 0.1475 - accuracy: 0.9710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb2aa2a160>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a fresh model\n",
    "\n",
    "model = create_model()\n",
    "model.fit(train_images, train_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a saved_model - place it in time-stamped directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0403 22:57:23.624114 140736985473984 deprecation.py:323] From /anaconda3/lib/python3.6/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:253: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "W0403 22:57:23.625437 140736985473984 tf_logging.py:161] Export includes no default signature!\n",
      "W0403 22:57:24.460882 140736985473984 tf_logging.py:161] Export includes no default signature!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./saved_models/1554357442'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "saved_model_path = './saved_models/{}'.format(int(time.time()))\n",
    "\n",
    "tf.keras.experimental.export_saved_model(model, saved_model_path)\n",
    "saved_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m1554357442\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "# List saved models\n",
    "\n",
    "!ls saved_models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload a new Keras model from the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.experimental.load_from_saved_model(saved_model_path)\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Restored Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_images).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 180us/sample - loss: 0.4351 - accuracy: 0.8590\n",
      "Restored model, accuracy: 85.90%\n"
     ]
    }
   ],
   "source": [
    "# Model needs to be Compiled before evaluation - not necessary with deployment alone\n",
    "\n",
    "new_model.compile(optimizer=model.optimizer,\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Evaluate restored model\n",
    "\n",
    "loss, acc = new_model.evaluate(test_images, test_labels)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Notes\n",
    "#### Entire models can be saved as HDF5 files, or by using saved_model method\n",
    "#### Weights and Checkpoints can also be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support for this exercise can be found at tensorflow.org - Permissions granted\n",
    "#\n",
    "#@title MIT License\n",
    "#\n",
    "# Copyright (c) 2017 François Chollet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
